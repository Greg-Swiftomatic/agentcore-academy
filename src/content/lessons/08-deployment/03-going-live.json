{
  "title": "Going Live: Production Checklist",
  "objectives": [
    "Complete a comprehensive pre-launch checklist covering security, monitoring, and operations",
    "Establish runbooks and incident response procedures before launch",
    "Plan for graceful degradation and failure scenarios"
  ],
  "content": "# Going Live: Production Checklist\n\nLaunching an agent to production is more than just deploying code. This lesson covers everything you need to verify before, during, and after going live.\n\n## Pre-Launch Checklist\n\n### Security\n\n```\n[ ] IAM & Permissions\n    [ ] Runtime execution role follows least privilege\n    [ ] Trust policy locked to bedrock-agentcore.amazonaws.com\n    [ ] Permission boundaries set\n    [ ] No wildcard (*) permissions unless absolutely necessary\n    [ ] Secrets stored in Secrets Manager (not env vars or code)\n\n[ ] Network\n    [ ] VPC endpoints configured for AWS services\n    [ ] Security groups restrict unnecessary access\n    [ ] No hardcoded IPs or credentials\n\n[ ] Data Protection\n    [ ] Encryption at rest enabled (S3, DynamoDB, etc.)\n    [ ] TLS 1.2+ enforced for all connections\n    [ ] PII handling compliant with policy\n    [ ] Data retention policies configured\n\n[ ] Authentication\n    [ ] Inbound auth configured (how callers authenticate)\n    [ ] Token validation implemented\n    [ ] Rate limiting in place\n```\n\n### Monitoring & Observability\n\n```\n[ ] Metrics\n    [ ] CloudWatch dashboard created\n    [ ] Key metrics tracked: invocations, latency, errors, tokens\n    [ ] Custom business metrics defined\n\n[ ] Alerts\n    [ ] Error rate alert configured\n    [ ] Latency (p95) alert configured\n    [ ] Cost anomaly detection enabled\n    [ ] Alert routing to on-call configured\n\n[ ] Logging\n    [ ] Structured logging implemented\n    [ ] Log levels appropriate (not all DEBUG)\n    [ ] Sensitive data redacted from logs\n    [ ] Log retention policy set\n\n[ ] Tracing\n    [ ] OpenTelemetry/X-Ray configured\n    [ ] All tools instrumented\n    [ ] Trace sampling rate appropriate\n```\n\n### Reliability\n\n```\n[ ] Error Handling\n    [ ] Retry logic with exponential backoff\n    [ ] Circuit breakers for external dependencies\n    [ ] Graceful degradation for non-critical features\n    [ ] User-friendly error messages (no stack traces)\n\n[ ] Capacity\n    [ ] Load tested at expected peak traffic\n    [ ] Auto-scaling configured and tested\n    [ ] Rate limits set appropriately\n    [ ] Timeout values tuned\n\n[ ] Resilience\n    [ ] Tested behavior when LLM is unavailable\n    [ ] Tested behavior when tools fail\n    [ ] Tested behavior under high load\n    [ ] Recovery procedures documented\n```\n\n### Testing\n\n```\n[ ] Test Coverage\n    [ ] Unit tests passing\n    [ ] Integration tests passing\n    [ ] Behavioral tests passing\n    [ ] Performance tests meet thresholds\n\n[ ] Production Readiness\n    [ ] Smoke tests defined and automated\n    [ ] Rollback procedure tested\n    [ ] Staging environment mirrors production\n```\n\n### Operations\n\n```\n[ ] Documentation\n    [ ] Runbook for common issues\n    [ ] Architecture diagram current\n    [ ] On-call rotation defined\n    [ ] Escalation path documented\n\n[ ] Deployment\n    [ ] CI/CD pipeline tested\n    [ ] Deployment strategy defined (canary/blue-green)\n    [ ] Feature flags for risky changes\n    [ ] Rollback procedure documented and tested\n```\n\n## Runbook Template\n\nCreate runbooks for predictable issues:\n\n```markdown\n# Runbook: High Error Rate\n\n## Symptoms\n- Error rate > 5% for 5+ minutes\n- Alert: \"AgentCore-HighErrorRate\" firing\n\n## Initial Assessment (2 minutes)\n1. Check CloudWatch dashboard for error spike timing\n2. Check recent deployments (was anything deployed in last hour?)\n3. Check dependent service status (Bedrock, external APIs)\n\n## Triage Decision Tree\n\n### If error started with deployment:\n1. Initiate rollback: `./scripts/rollback.sh`\n2. Notify team in #agent-ops\n3. Investigate cause post-rollback\n\n### If error is from external dependency:\n1. Check AWS status page for Bedrock\n2. Check external API status pages\n3. If Bedrock: enable fallback model or return cached responses\n4. If external API: enable circuit breaker if not already tripped\n\n### If error is from traffic spike:\n1. Check if scaling is occurring\n2. If at max capacity: enable load shedding\n3. Investigate traffic source (legitimate or attack?)\n\n## Escalation\n- If not resolved in 15 minutes: page secondary on-call\n- If customer-impacting: notify customer success team\n- If security-related: page security on-call\n\n## Post-Incident\n- Create incident ticket\n- Schedule post-mortem within 48 hours\n- Update runbook if new scenario discovered\n```\n\n## Launch Day Procedure\n\n### Pre-Launch (T-1 hour)\n\n```python\n# Automated pre-launch checks\ndef pre_launch_verification():\n    checks = [\n        (\"Staging smoke tests\", run_smoke_tests, \"staging\"),\n        (\"Production config valid\", validate_config, \"production\"),\n        (\"Secrets accessible\", verify_secrets_access),\n        (\"Monitoring active\", verify_monitoring),\n        (\"Rollback tested\", verify_rollback_ready),\n        (\"On-call confirmed\", verify_oncall_schedule),\n    ]\n    \n    all_passed = True\n    for name, check_fn, *args in checks:\n        try:\n            result = check_fn(*args)\n            status = \"PASS\" if result else \"FAIL\"\n            all_passed = all_passed and result\n        except Exception as e:\n            status = f\"ERROR: {e}\"\n            all_passed = False\n        \n        print(f\"[{status}] {name}\")\n    \n    return all_passed\n\nif not pre_launch_verification():\n    print(\"ABORT: Pre-launch checks failed\")\n    sys.exit(1)\n```\n\n### Launch (T-0)\n\n```bash\n# Canary deployment to production\n./scripts/canary_deploy.sh \\\n  --version $VERSION \\\n  --stages 5,10,25,50,100 \\\n  --monitor-minutes 10 \\\n  --rollback-on-error-rate 0.05\n```\n\n### Post-Launch (T+1 hour)\n\n```python\ndef post_launch_verification():\n    \"\"\"Verify launch success after 1 hour.\"\"\"\n    \n    metrics = get_metrics(last_hours=1)\n    \n    checks = {\n        \"Error rate < 1%\": metrics.error_rate < 0.01,\n        \"P95 latency < 5s\": metrics.p95_latency < 5000,\n        \"Successful invocations > 0\": metrics.invocations > 0,\n        \"No critical alerts\": len(get_critical_alerts()) == 0,\n    }\n    \n    for check, passed in checks.items():\n        status = \"PASS\" if passed else \"FAIL\"\n        print(f\"[{status}] {check}\")\n    \n    return all(checks.values())\n```\n\n## Gradual Rollout Strategy\n\n```\n┌────────────────────────────────────────────────────────────────┐\n│                    Gradual Rollout Stages                       │\n│                                                                 │\n│  Stage 1: Internal Users (Day 1)                               │\n│  - Deploy to production                                         │\n│  - Route only internal/employee traffic                         │\n│  - Heavy monitoring, fast rollback trigger                      │\n│                                                                 │\n│  Stage 2: Beta Users (Day 2-3)                                 │\n│  - Add opt-in beta customers                                    │\n│  - Gather feedback actively                                     │\n│  - Fix issues found                                             │\n│                                                                 │\n│  Stage 3: Percentage Rollout (Day 4-7)                         │\n│  - 10% → 25% → 50% → 75% of traffic                            │\n│  - Each stage: 24 hours minimum                                 │\n│  - Monitor error rates and latency                              │\n│                                                                 │\n│  Stage 4: General Availability (Day 8+)                        │\n│  - 100% of traffic                                              │\n│  - Continue monitoring                                          │\n│  - Document learnings                                           │\n└────────────────────────────────────────────────────────────────┘\n```\n\n## Incident Response Preparation\n\n### Severity Levels\n\n| Severity | Description | Response Time | Example |\n|----------|-------------|---------------|--------|\n| SEV-1 | Complete outage | 15 minutes | Agent not responding |\n| SEV-2 | Major degradation | 1 hour | 50%+ error rate |\n| SEV-3 | Minor issue | 4 hours | Elevated latency |\n| SEV-4 | Low impact | Next business day | Logging not working |\n\n### Communication Template\n\n```markdown\n# Incident Update: [Title]\n\n**Status**: Investigating / Identified / Monitoring / Resolved\n**Impact**: [What users experience]\n**Start Time**: [When it started]\n\n**Current State**:\n[Brief description of current situation]\n\n**Actions Taken**:\n- [Action 1]\n- [Action 2]\n\n**Next Update**: [Time or \"when we have more information\"]\n```\n\n## Post-Launch Monitoring (First 30 Days)\n\n### Week 1: Intensive Monitoring\n- Check dashboards every 2 hours\n- Review all alerts (even resolved ones)\n- Gather user feedback actively\n- Be ready to rollback quickly\n\n### Week 2-4: Stabilization\n- Daily dashboard review\n- Weekly metrics summary\n- Address feedback and minor issues\n- Fine-tune alerts (reduce noise)\n\n### Ongoing\n- Weekly metrics review\n- Monthly capacity review\n- Quarterly security review\n\n## Summary\n\nGoing live successfully requires:\n\n1. **Complete the checklist** - Security, monitoring, reliability, testing, operations\n2. **Prepare runbooks** - Document what to do when things go wrong\n3. **Launch gradually** - Internal → Beta → Percentage → GA\n4. **Plan for incidents** - Severity levels, response times, communication\n5. **Monitor intensively** - Especially in the first week\n\nThe goal isn't a perfect launch - it's a launch where you can quickly detect and recover from any issue."
}
