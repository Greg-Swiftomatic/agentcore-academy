{
  "title": "Scaling Strategies",
  "objectives": [
    "Identify bottlenecks and capacity limits in agent deployments",
    "Implement horizontal scaling patterns for high-throughput scenarios",
    "Optimize for both latency and cost at scale"
  ],
  "content": "# Scaling Agent Infrastructure\n\nAs your agent usage grows, you'll encounter capacity limits. AgentCore's serverless architecture handles much of this automatically, but understanding scaling patterns helps you optimize for your specific needs.\n\n## Understanding Capacity Limits\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    Where Bottlenecks Occur                       │\n│                                                                  │\n│  ┌─────────────────────────────────────────────────────────────┐│\n│  │ LLM Model Throughput                                        ││\n│  │ - Tokens per minute limits                                  ││\n│  │ - Concurrent request limits                                 ││\n│  │ - Often the primary bottleneck                              ││\n│  └─────────────────────────────────────────────────────────────┘│\n│                                                                  │\n│  ┌─────────────────────────────────────────────────────────────┐│\n│  │ Runtime Concurrency                                         ││\n│  │ - Concurrent agent sessions                                 ││\n│  │ - Memory per session                                        ││\n│  │ - AgentCore handles auto-scaling                            ││\n│  └─────────────────────────────────────────────────────────────┘│\n│                                                                  │\n│  ┌─────────────────────────────────────────────────────────────┐│\n│  │ External Dependencies                                       ││\n│  │ - Database connections                                      ││\n│  │ - Third-party API rate limits                               ││\n│  │ - Often overlooked until production                         ││\n│  └─────────────────────────────────────────────────────────────┘│\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## AgentCore Auto-Scaling\n\nAgentCore Runtime automatically scales based on demand:\n\n```python\n# Configuration for auto-scaling behavior\nagent_config = {\n    \"runtime\": {\n        \"min_instances\": 1,          # Always keep 1 warm\n        \"max_instances\": 100,        # Scale up to 100\n        \"scale_up_threshold\": 0.7,   # Add capacity at 70% utilization\n        \"scale_down_delay\": 300,     # Wait 5 min before scaling down\n        \"memory_mb\": 512,            # Per-instance memory\n    }\n}\n\n# AgentCore handles:\n# - Provisioning new instances when load increases\n# - Distributing requests across instances\n# - Scaling down during low traffic\n# - Health checks and instance replacement\n```\n\n## Scaling Patterns\n\n### 1. Request Queuing\n\nBuffer traffic spikes with a queue:\n\n```python\nimport boto3\n\nsqs = boto3.client('sqs')\n\n# Producer: Queue incoming requests\nasync def queue_request(request: dict):\n    sqs.send_message(\n        QueueUrl=AGENT_QUEUE_URL,\n        MessageBody=json.dumps(request),\n        MessageAttributes={\n            'Priority': {\n                'DataType': 'String',\n                'StringValue': request.get('priority', 'normal')\n            }\n        }\n    )\n    return {\"status\": \"queued\", \"request_id\": request[\"id\"]}\n\n# Consumer: Process from queue\nasync def process_queue():\n    while True:\n        messages = sqs.receive_message(\n            QueueUrl=AGENT_QUEUE_URL,\n            MaxNumberOfMessages=10,\n            WaitTimeSeconds=20\n        )\n        \n        for msg in messages.get('Messages', []):\n            request = json.loads(msg['Body'])\n            try:\n                result = await agent.invoke(request)\n                await notify_completion(request['id'], result)\n            finally:\n                sqs.delete_message(\n                    QueueUrl=AGENT_QUEUE_URL,\n                    ReceiptHandle=msg['ReceiptHandle']\n                )\n```\n\n### 2. Load Shedding\n\nReject requests gracefully when overloaded:\n\n```python\nfrom asyncio import Semaphore\n\nclass LoadShedder:\n    def __init__(self, max_concurrent: int = 50, queue_size: int = 100):\n        self.semaphore = Semaphore(max_concurrent)\n        self.queue_size = queue_size\n        self.waiting = 0\n    \n    async def process(self, request):\n        if self.waiting >= self.queue_size:\n            raise ServiceOverloaded(\n                \"System at capacity. Please retry in 30 seconds.\",\n                retry_after=30\n            )\n        \n        self.waiting += 1\n        try:\n            async with self.semaphore:\n                return await agent.invoke(request)\n        finally:\n            self.waiting -= 1\n\nshedder = LoadShedder(max_concurrent=50)\n\n@app.post(\"/chat\")\nasync def chat(request: ChatRequest):\n    try:\n        return await shedder.process(request)\n    except ServiceOverloaded as e:\n        return JSONResponse(\n            status_code=503,\n            content={\"error\": str(e), \"retry_after\": e.retry_after}\n        )\n```\n\n### 3. Priority Queues\n\nProcess high-priority requests first:\n\n```python\nimport heapq\nfrom asyncio import PriorityQueue\n\nPRIORITY_LEVELS = {\n    \"critical\": 0,   # Highest priority\n    \"high\": 1,\n    \"normal\": 2,\n    \"low\": 3         # Lowest priority\n}\n\nclass PriorityScheduler:\n    def __init__(self, workers: int = 10):\n        self.queue = PriorityQueue()\n        self.workers = workers\n    \n    async def enqueue(self, request: dict, priority: str = \"normal\"):\n        priority_value = PRIORITY_LEVELS.get(priority, 2)\n        await self.queue.put((priority_value, time.time(), request))\n    \n    async def process_loop(self):\n        while True:\n            priority, timestamp, request = await self.queue.get()\n            try:\n                await agent.invoke(request)\n            finally:\n                self.queue.task_done()\n\n# Critical requests processed before normal ones\nscheduler = PriorityScheduler()\nawait scheduler.enqueue(request, priority=\"critical\")\n```\n\n### 4. Model Routing for Scale\n\nDistribute load across models:\n\n```python\nclass ModelRouter:\n    def __init__(self):\n        self.models = {\n            \"fast\": {\n                \"id\": \"claude-3-haiku\",\n                \"max_tpm\": 100000,   # Tokens per minute\n                \"current_tpm\": 0\n            },\n            \"balanced\": {\n                \"id\": \"claude-3-sonnet\",\n                \"max_tpm\": 50000,\n                \"current_tpm\": 0\n            },\n            \"powerful\": {\n                \"id\": \"claude-3-opus\",\n                \"max_tpm\": 20000,\n                \"current_tpm\": 0\n            }\n        }\n    \n    def select_model(self, request: dict) -> str:\n        complexity = estimate_complexity(request)\n        \n        # Try preferred model first, fall back if at capacity\n        preferred = self._get_preferred_model(complexity)\n        \n        for model_tier in self._get_fallback_order(preferred):\n            model = self.models[model_tier]\n            if model[\"current_tpm\"] < model[\"max_tpm\"] * 0.8:\n                return model[\"id\"]\n        \n        # All models at capacity\n        raise CapacityExceeded(\"All models at capacity\")\n    \n    def _get_preferred_model(self, complexity: str) -> str:\n        mapping = {\n            \"simple\": \"fast\",\n            \"medium\": \"balanced\",\n            \"complex\": \"powerful\"\n        }\n        return mapping.get(complexity, \"balanced\")\n```\n\n## Caching at Scale\n\n```python\nimport redis\nimport hashlib\n\nclass ResponseCache:\n    def __init__(self, redis_url: str):\n        self.redis = redis.from_url(redis_url)\n        self.default_ttl = 3600  # 1 hour\n    \n    def _cache_key(self, prompt: str, context: dict) -> str:\n        \"\"\"Generate deterministic cache key.\"\"\"\n        content = f\"{prompt}:{json.dumps(context, sort_keys=True)}\"\n        return f\"agent:response:{hashlib.sha256(content.encode()).hexdigest()}\"\n    \n    async def get_or_compute(self, prompt: str, context: dict, compute_fn):\n        key = self._cache_key(prompt, context)\n        \n        # Try cache first\n        cached = self.redis.get(key)\n        if cached:\n            return json.loads(cached)\n        \n        # Compute and cache\n        result = await compute_fn(prompt, context)\n        self.redis.setex(key, self.default_ttl, json.dumps(result))\n        return result\n\ncache = ResponseCache(\"redis://localhost:6379\")\n\n# Usage\nresponse = await cache.get_or_compute(\n    prompt=request.prompt,\n    context={\"user_id\": request.user_id},\n    compute_fn=agent.invoke\n)\n```\n\n## Multi-Region Deployment\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    Multi-Region Architecture                     │\n│                                                                  │\n│  ┌──────────────┐   ┌──────────────┐   ┌──────────────┐         │\n│  │   us-east-1  │   │   eu-west-1  │   │  ap-south-1  │         │\n│  │              │   │              │   │              │         │\n│  │ ┌──────────┐ │   │ ┌──────────┐ │   │ ┌──────────┐ │         │\n│  │ │AgentCore │ │   │ │AgentCore │ │   │ │AgentCore │ │         │\n│  │ │ Runtime  │ │   │ │ Runtime  │ │   │ │ Runtime  │ │         │\n│  │ └──────────┘ │   │ └──────────┘ │   │ └──────────┘ │         │\n│  │              │   │              │   │              │         │\n│  │ ┌──────────┐ │   │ ┌──────────┐ │   │ ┌──────────┐ │         │\n│  │ │  Bedrock │ │   │ │  Bedrock │ │   │ │  Bedrock │ │         │\n│  │ │  Models  │ │   │ │  Models  │ │   │ │  Models  │ │         │\n│  │ └──────────┘ │   │ └──────────┘ │   │ └──────────┘ │         │\n│  └──────────────┘   └──────────────┘   └──────────────┘         │\n│                                                                  │\n│            Route 53 / CloudFront (Latency-based routing)         │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n```python\n# Terraform configuration for multi-region\nresource \"aws_route53_record\" \"agent_api\" {\n  zone_id = aws_route53_zone.main.zone_id\n  name    = \"agent-api.example.com\"\n  type    = \"A\"\n\n  latency_routing_policy {\n    region = \"us-east-1\"\n  }\n\n  alias {\n    name                   = aws_lb.us_east.dns_name\n    zone_id                = aws_lb.us_east.zone_id\n    evaluate_target_health = true\n  }\n}\n\n# Repeat for each region...\n```\n\n## Capacity Planning\n\n```python\ndef estimate_capacity_needs(\n    expected_requests_per_day: int,\n    avg_tokens_per_request: int,\n    peak_multiplier: float = 3.0\n) -> dict:\n    \"\"\"Estimate infrastructure needs.\"\"\"\n    \n    daily_tokens = expected_requests_per_day * avg_tokens_per_request\n    peak_requests_per_minute = (expected_requests_per_day / 1440) * peak_multiplier\n    peak_tokens_per_minute = peak_requests_per_minute * avg_tokens_per_request\n    \n    # Model throughput limits (example)\n    haiku_tpm = 100000\n    sonnet_tpm = 50000\n    \n    return {\n        \"daily_tokens\": daily_tokens,\n        \"peak_rpm\": peak_requests_per_minute,\n        \"peak_tpm\": peak_tokens_per_minute,\n        \"haiku_instances_needed\": math.ceil(peak_tokens_per_minute / haiku_tpm),\n        \"sonnet_instances_needed\": math.ceil(peak_tokens_per_minute / sonnet_tpm),\n        \"estimated_daily_cost\": estimate_cost(daily_tokens)\n    }\n\n# Example\nneeds = estimate_capacity_needs(\n    expected_requests_per_day=100000,\n    avg_tokens_per_request=1000\n)\nprint(f\"Peak TPM needed: {needs['peak_tpm']:,.0f}\")\nprint(f\"Haiku instances: {needs['haiku_instances_needed']}\")\n```\n\n## Scaling Checklist\n\n```\n[ ] Understand Limits\n    [ ] LLM token/request limits documented\n    [ ] External API rate limits known\n    [ ] Database connection limits configured\n\n[ ] Handle Overload\n    [ ] Request queuing implemented\n    [ ] Load shedding with graceful errors\n    [ ] Priority routing for critical requests\n\n[ ] Optimize Throughput\n    [ ] Response caching enabled\n    [ ] Model routing by complexity\n    [ ] Connection pooling for databases\n\n[ ] Monitor Scale\n    [ ] Capacity utilization dashboards\n    [ ] Alerts before hitting limits\n    [ ] Cost tracking per request\n\n[ ] Plan for Growth\n    [ ] Multi-region if latency-sensitive\n    [ ] Capacity planning model\n    [ ] Load testing before launches\n```\n\n## Summary\n\nScaling agent infrastructure:\n\n1. **Understand bottlenecks** - LLM throughput is usually the limit, not compute\n2. **Queue traffic** - Buffer spikes, don't reject immediately\n3. **Shed load gracefully** - Better to reject some than fail all\n4. **Cache aggressively** - Same prompt = same response\n5. **Route by complexity** - Use cheaper models for simple requests\n6. **Plan capacity** - Model your peak load before it arrives\n\nAgentCore handles auto-scaling, but you control how traffic reaches it."
}
