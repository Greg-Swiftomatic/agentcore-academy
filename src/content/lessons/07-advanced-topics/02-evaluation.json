{
  "title": "Evaluating Agent Performance",
  "objectives": [
    "Define meaningful metrics for agent quality beyond basic accuracy",
    "Implement automated evaluation pipelines using LLM-as-judge and reference-based methods",
    "Build continuous evaluation into agent development workflows"
  ],
  "content": "# Evaluating Agent Performance\n\nHow do you know if your agent is good? Unlike traditional software with clear pass/fail tests, agent quality is nuanced and multidimensional. Evaluation is critical for improvement.\n\n## The Evaluation Challenge\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│            Why Agent Evaluation is Hard                          │\n│                                                                  │\n│  Traditional Software          AI Agents                         │\n│  ├── Deterministic            ├── Non-deterministic              │\n│  ├── Clear pass/fail          ├── Quality is a spectrum          │\n│  ├── Unit testable            ├── Outputs are open-ended         │\n│  └── Behavior is specified    └── Behavior emerges from prompts  │\n│                                                                  │\n│  Traditional Testing: \"Does add(2,3) return 5?\"                  │\n│  Agent Evaluation: \"Is this response helpful, accurate, safe?\"   │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Evaluation Dimensions\n\n### 1. Correctness\n\nDoes the agent produce accurate outputs?\n\n```python\ndef evaluate_correctness(response: str, ground_truth: str) -> float:\n    \"\"\"Compare agent response to known correct answer.\"\"\"\n    \n    # Exact match (strict)\n    if response.strip() == ground_truth.strip():\n        return 1.0\n    \n    # Semantic similarity (flexible)\n    embeddings = embed([response, ground_truth])\n    similarity = cosine_similarity(embeddings[0], embeddings[1])\n    \n    return similarity\n```\n\n### 2. Helpfulness\n\nDoes the agent actually help the user accomplish their goal?\n\n```python\ndef evaluate_helpfulness(query: str, response: str) -> dict:\n    \"\"\"Use LLM to judge helpfulness.\"\"\"\n    \n    judge_prompt = f\"\"\"\n    Query: {query}\n    Response: {response}\n    \n    Rate the response on:\n    1. Relevance (1-5): Does it address the query?\n    2. Completeness (1-5): Does it fully answer the question?\n    3. Actionability (1-5): Can the user act on this information?\n    \n    Return JSON: {{\"relevance\": N, \"completeness\": N, \"actionability\": N}}\n    \"\"\"\n    \n    return llm_judge(judge_prompt)\n```\n\n### 3. Safety\n\nDoes the agent avoid harmful outputs?\n\n```python\nSAFETY_CATEGORIES = [\n    \"harmful_instructions\",\n    \"personal_info_exposure\",\n    \"biased_content\",\n    \"policy_violations\"\n]\n\ndef evaluate_safety(response: str) -> dict:\n    \"\"\"Check response for safety issues.\"\"\"\n    \n    results = {}\n    for category in SAFETY_CATEGORIES:\n        classifier = get_safety_classifier(category)\n        results[category] = classifier.predict(response)\n    \n    return {\n        \"safe\": all(r[\"safe\"] for r in results.values()),\n        \"details\": results\n    }\n```\n\n### 4. Efficiency\n\nDoes the agent accomplish goals with reasonable resources?\n\n```python\ndef evaluate_efficiency(trace: AgentTrace) -> dict:\n    \"\"\"Analyze agent efficiency from execution trace.\"\"\"\n    \n    return {\n        \"total_tokens\": trace.total_tokens,\n        \"tool_calls\": len(trace.tool_invocations),\n        \"unnecessary_calls\": count_redundant_calls(trace),\n        \"reasoning_loops\": count_reasoning_loops(trace),\n        \"time_to_answer\": trace.duration_seconds\n    }\n```\n\n## Evaluation Methods\n\n### 1. Reference-Based Evaluation\n\nCompare against known correct answers:\n\n```python\nfrom agentcore.eval import ReferenceEvaluator\n\n# Create test dataset\ntest_cases = [\n    {\n        \"input\": \"What's the capital of France?\",\n        \"expected\": \"Paris\",\n        \"match_type\": \"contains\"  # Response should contain \"Paris\"\n    },\n    {\n        \"input\": \"Convert 100 USD to EUR\",\n        \"expected_pattern\": r\"\\d+\\.\\d{2} EUR\",  # Regex pattern\n        \"match_type\": \"regex\"\n    }\n]\n\nevaluator = ReferenceEvaluator(test_cases)\nresults = await evaluator.run(agent)\n\nprint(f\"Accuracy: {results.accuracy}\")\nprint(f\"Failures: {results.failures}\")\n```\n\n### 2. LLM-as-Judge\n\nUse another LLM to evaluate quality:\n\n```python\nfrom agentcore.eval import LLMJudge\n\njudge = LLMJudge(\n    model=\"claude-3-sonnet\",  # Judge model\n    criteria=[\n        \"accuracy\",\n        \"helpfulness\", \n        \"clarity\",\n        \"safety\"\n    ],\n    rubric=\"\"\"\n    Score each criterion from 1-5:\n    5 = Excellent, exceeds expectations\n    4 = Good, meets expectations\n    3 = Acceptable, minor issues\n    2 = Poor, significant issues\n    1 = Unacceptable, fails criterion\n    \"\"\"\n)\n\n# Evaluate a response\nscore = await judge.evaluate(\n    query=\"Explain quantum computing\",\n    response=agent_response\n)\n\nprint(f\"Scores: {score.criteria_scores}\")\nprint(f\"Overall: {score.overall}\")\nprint(f\"Reasoning: {score.explanation}\")\n```\n\n### 3. Human Evaluation\n\nCollect ratings from real users or annotators:\n\n```python\nfrom agentcore.eval import HumanEvalPipeline\n\npipeline = HumanEvalPipeline(\n    sample_size=100,  # Random sample of interactions\n    questions=[\n        {\n            \"id\": \"helpful\",\n            \"text\": \"Was this response helpful?\",\n            \"type\": \"likert_5\"\n        },\n        {\n            \"id\": \"would_use_again\",\n            \"text\": \"Would you ask this agent again?\",\n            \"type\": \"yes_no\"\n        },\n        {\n            \"id\": \"feedback\",\n            \"text\": \"Any additional feedback?\",\n            \"type\": \"free_text\"\n        }\n    ]\n)\n\n# Send to annotation platform\npipeline.create_annotation_batch(agent_responses)\n\n# Retrieve results\nhuman_scores = await pipeline.get_results()\n```\n\n### 4. A/B Testing\n\nCompare agent versions with real traffic:\n\n```python\nfrom agentcore.eval import ABTest\n\ntest = ABTest(\n    control=agent_v1,\n    treatment=agent_v2,\n    traffic_split=0.5,  # 50/50 split\n    metrics=[\n        \"task_success_rate\",\n        \"user_satisfaction\",\n        \"avg_turns_to_resolution\"\n    ],\n    min_sample_size=1000\n)\n\n# Route traffic\n@app.route(\"/chat\")\nasync def chat(request):\n    agent = test.get_variant(user_id=request.user_id)\n    response = await agent.invoke(request.message)\n    test.log_interaction(user_id, response, request.feedback)\n    return response\n\n# Check results\nresults = test.analyze()\nprint(f\"Winner: {results.winner}\")\nprint(f\"Confidence: {results.statistical_significance}\")\n```\n\n## Building an Evaluation Pipeline\n\n```python\nfrom agentcore.eval import EvaluationPipeline\n\n# Define comprehensive evaluation\npipeline = EvaluationPipeline(\n    evaluators=[\n        ReferenceEvaluator(test_cases),      # Correctness\n        LLMJudge(criteria=[\"helpfulness\"]),   # Quality\n        SafetyEvaluator(),                    # Safety\n        EfficiencyEvaluator()                 # Efficiency\n    ],\n    aggregation=\"weighted_average\",\n    weights={\n        \"correctness\": 0.4,\n        \"helpfulness\": 0.3,\n        \"safety\": 0.2,\n        \"efficiency\": 0.1\n    }\n)\n\n# Run evaluation\nresults = await pipeline.evaluate(agent, dataset)\n\n# Generate report\nreport = pipeline.generate_report(results)\nprint(report.summary)\nprint(report.failure_analysis)\nprint(report.recommendations)\n```\n\n## Continuous Evaluation\n\n### Integrate with CI/CD\n\n```yaml\n# .github/workflows/agent-eval.yml\nname: Agent Evaluation\n\non:\n  pull_request:\n    paths:\n      - 'agents/**'\n      - 'prompts/**'\n\njobs:\n  evaluate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Run Evaluation Suite\n        run: python -m agentcore.eval run --config eval_config.yaml\n        \n      - name: Check Quality Gates\n        run: |\n          python -m agentcore.eval check \\\n            --min-accuracy 0.85 \\\n            --min-safety 0.99 \\\n            --max-regression 0.05\n```\n\n### Production Monitoring\n\n```python\nfrom agentcore.eval import ProductionMonitor\n\nmonitor = ProductionMonitor(\n    sample_rate=0.01,  # Evaluate 1% of traffic\n    evaluators=[LLMJudge(), SafetyEvaluator()],\n    alerts=[\n        {\"metric\": \"safety_score\", \"threshold\": 0.95, \"action\": \"page\"},\n        {\"metric\": \"helpfulness\", \"threshold\": 3.0, \"action\": \"email\"}\n    ]\n)\n\n@app.middleware\nasync def evaluation_middleware(request, call_next):\n    response = await call_next(request)\n    await monitor.maybe_evaluate(request, response)  # Async, non-blocking\n    return response\n```\n\n## Common Evaluation Mistakes\n\n| Mistake | Why It's Bad | Better Approach |\n|---------|--------------|----------------|\n| Only testing happy paths | Misses edge cases and failures | Include adversarial examples |\n| Single metric focus | Gaming one metric hurts others | Multi-dimensional evaluation |\n| Infrequent evaluation | Catches issues too late | Continuous evaluation |\n| No baseline comparison | Can't measure improvement | Always compare to baseline |\n| Ignoring safety | Dangerous in production | Safety as hard gate |\n\n## Evaluation Metrics Dashboard\n\n```python\n# Track key metrics over time\nmetrics = {\n    \"correctness\": {\n        \"current\": 0.87,\n        \"baseline\": 0.82,\n        \"target\": 0.90,\n        \"trend\": \"improving\"\n    },\n    \"helpfulness\": {\n        \"current\": 4.2,\n        \"baseline\": 4.0,\n        \"target\": 4.5,\n        \"trend\": \"stable\"\n    },\n    \"safety\": {\n        \"current\": 0.99,\n        \"baseline\": 0.99,\n        \"target\": 0.999,\n        \"trend\": \"stable\"\n    },\n    \"efficiency\": {\n        \"current\": 850,  # avg tokens\n        \"baseline\": 1200,\n        \"target\": 500,\n        \"trend\": \"improving\"\n    }\n}\n```\n\n## Summary\n\nAgent evaluation requires:\n\n1. **Multiple dimensions** - Correctness, helpfulness, safety, efficiency\n2. **Multiple methods** - Reference-based, LLM-as-judge, human evaluation, A/B tests\n3. **Continuous process** - Not one-time, but integrated into development\n4. **Clear baselines** - Always compare against something\n5. **Safety as gate** - Hard requirements, not just metrics\n\nYou can't improve what you don't measure. Invest in evaluation infrastructure early."
}
