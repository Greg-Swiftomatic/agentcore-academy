{
  "title": "Error Handling & Resilience",
  "objectives": [
    "Implement retry strategies with exponential backoff for transient failures",
    "Design graceful degradation for agent capabilities",
    "Build circuit breakers to prevent cascade failures"
  ],
  "content": "# Error Handling & Resilience\n\nProduction agents encounter failures: APIs timeout, models throttle, tools return errors. Resilient agents handle these gracefully without failing the user.\n\n## Types of Failures\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                      Failure Categories                          │\n│                                                                  │\n│  TRANSIENT (retry-able)           PERMANENT (don't retry)       │\n│  ├── Network timeouts             ├── Invalid input             │\n│  ├── Rate limiting (429)          ├── Authentication failed     │\n│  ├── Service unavailable (503)    ├── Resource not found (404)  │\n│  └── Temporary model overload     └── Permission denied (403)   │\n│                                                                  │\n│  DEGRADABLE (fallback available)  CRITICAL (must fail)          │\n│  ├── Optional tool unavailable    ├── Core tool unavailable     │\n│  ├── Enhanced feature down        ├── Model completely down     │\n│  └── Cache miss                   └── Data corruption           │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Retry Strategies\n\n### Exponential Backoff\n\n```python\nimport time\nimport random\nfrom functools import wraps\n\ndef retry_with_backoff(\n    max_retries: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 60.0,\n    retryable_exceptions: tuple = (TimeoutError, ConnectionError)\n):\n    \"\"\"Retry decorator with exponential backoff and jitter.\"\"\"\n    \n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            last_exception = None\n            \n            for attempt in range(max_retries + 1):\n                try:\n                    return func(*args, **kwargs)\n                except retryable_exceptions as e:\n                    last_exception = e\n                    \n                    if attempt == max_retries:\n                        break\n                    \n                    # Exponential backoff with jitter\n                    delay = min(base_delay * (2 ** attempt), max_delay)\n                    delay = delay * (0.5 + random.random())  # Add jitter\n                    \n                    logger.warning(f\"Retry {attempt + 1}/{max_retries} after {delay:.2f}s: {e}\")\n                    time.sleep(delay)\n            \n            raise last_exception\n        return wrapper\n    return decorator\n\n# Usage\n@retry_with_backoff(max_retries=3, retryable_exceptions=(TimeoutError, RateLimitError))\ndef call_external_api(order_id: str):\n    return api_client.get_order(order_id)\n```\n\n### Retry for Model Invocations\n\n```python\nimport boto3\nfrom botocore.config import Config\n\n# Configure boto3 with built-in retry\nconfig = Config(\n    retries={\n        'max_attempts': 3,\n        'mode': 'adaptive'  # Adapts based on response\n    },\n    read_timeout=30,\n    connect_timeout=10\n)\n\nbedrock = boto3.client('bedrock-runtime', config=config)\n\n# For custom retry logic\ndef invoke_model_with_retry(prompt: str, max_retries: int = 3):\n    for attempt in range(max_retries):\n        try:\n            response = bedrock.invoke_model(\n                modelId='anthropic.claude-3-sonnet',\n                body=json.dumps({\"prompt\": prompt})\n            )\n            return response\n        except bedrock.exceptions.ThrottlingException:\n            if attempt < max_retries - 1:\n                time.sleep(2 ** attempt)  # 1s, 2s, 4s\n            else:\n                raise\n        except bedrock.exceptions.ModelTimeoutException:\n            if attempt < max_retries - 1:\n                time.sleep(1)\n            else:\n                raise\n```\n\n## Graceful Degradation\n\nWhen optional features fail, continue with reduced functionality:\n\n```python\nclass DegradableAgent:\n    def __init__(self):\n        self.tools = {\n            'core': ['get_order', 'update_order'],  # Must work\n            'enhanced': ['get_recommendations', 'analyze_sentiment']  # Nice to have\n        }\n    \n    def invoke(self, prompt: str, context: dict):\n        available_tools = self._get_available_tools()\n        \n        # Always include core tools\n        if not all(t in available_tools for t in self.tools['core']):\n            raise CriticalError(\"Core tools unavailable\")\n        \n        # Gracefully handle missing enhanced tools\n        missing_enhanced = [t for t in self.tools['enhanced'] if t not in available_tools]\n        if missing_enhanced:\n            logger.warning(f\"Degraded mode: {missing_enhanced} unavailable\")\n            context['degraded_mode'] = True\n            context['unavailable_tools'] = missing_enhanced\n        \n        return self._execute(prompt, context, available_tools)\n    \n    def _get_available_tools(self):\n        available = []\n        for tool in self.tools['core'] + self.tools['enhanced']:\n            try:\n                if self._health_check(tool):\n                    available.append(tool)\n            except Exception:\n                logger.warning(f\"Tool {tool} failed health check\")\n        return available\n```\n\n### Fallback Responses\n\n```python\ndef get_order_with_fallback(order_id: str):\n    \"\"\"Try primary source, fall back to cache, then to basic response.\"\"\"\n    \n    # Level 1: Try primary API\n    try:\n        return order_api.get_order(order_id)\n    except (TimeoutError, ServiceUnavailableError):\n        logger.warning(\"Primary API unavailable, trying cache\")\n    \n    # Level 2: Try cache\n    try:\n        cached = cache.get(f\"order:{order_id}\")\n        if cached:\n            return {**cached, \"_cached\": True, \"_cache_age\": cache.age(f\"order:{order_id}\")}\n    except Exception:\n        logger.warning(\"Cache unavailable\")\n    \n    # Level 3: Return minimal response\n    return {\n        \"order_id\": order_id,\n        \"status\": \"unknown\",\n        \"_degraded\": True,\n        \"_message\": \"Unable to retrieve order details. Please try again later.\"\n    }\n```\n\n## Circuit Breaker Pattern\n\nPrevent cascade failures by stopping calls to failing services:\n\n```python\nimport time\nfrom enum import Enum\nfrom threading import Lock\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"      # Normal operation\n    OPEN = \"open\"          # Failing, reject calls\n    HALF_OPEN = \"half_open\"  # Testing if recovered\n\nclass CircuitBreaker:\n    def __init__(\n        self,\n        failure_threshold: int = 5,\n        recovery_timeout: int = 30,\n        success_threshold: int = 2\n    ):\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.success_threshold = success_threshold\n        \n        self.state = CircuitState.CLOSED\n        self.failure_count = 0\n        self.success_count = 0\n        self.last_failure_time = None\n        self.lock = Lock()\n    \n    def call(self, func, *args, **kwargs):\n        with self.lock:\n            if self.state == CircuitState.OPEN:\n                if time.time() - self.last_failure_time > self.recovery_timeout:\n                    self.state = CircuitState.HALF_OPEN\n                    self.success_count = 0\n                else:\n                    raise CircuitOpenError(\"Circuit breaker is open\")\n        \n        try:\n            result = func(*args, **kwargs)\n            self._on_success()\n            return result\n        except Exception as e:\n            self._on_failure()\n            raise\n    \n    def _on_success(self):\n        with self.lock:\n            if self.state == CircuitState.HALF_OPEN:\n                self.success_count += 1\n                if self.success_count >= self.success_threshold:\n                    self.state = CircuitState.CLOSED\n                    self.failure_count = 0\n            else:\n                self.failure_count = 0\n    \n    def _on_failure(self):\n        with self.lock:\n            self.failure_count += 1\n            self.last_failure_time = time.time()\n            \n            if self.state == CircuitState.HALF_OPEN:\n                self.state = CircuitState.OPEN\n            elif self.failure_count >= self.failure_threshold:\n                self.state = CircuitState.OPEN\n\n# Usage\norder_circuit = CircuitBreaker(failure_threshold=5, recovery_timeout=30)\n\ndef get_order(order_id: str):\n    try:\n        return order_circuit.call(order_api.get, order_id)\n    except CircuitOpenError:\n        # Fall back to cached/degraded response\n        return get_cached_order(order_id)\n```\n\n## Error Response Handling\n\nReturn helpful errors to users, not technical failures:\n\n```python\nclass AgentError(Exception):\n    def __init__(self, user_message: str, internal_message: str, retry_after: int = None):\n        self.user_message = user_message\n        self.internal_message = internal_message\n        self.retry_after = retry_after\n\ndef handle_agent_error(error: Exception) -> dict:\n    \"\"\"Convert internal errors to user-friendly responses.\"\"\"\n    \n    if isinstance(error, RateLimitError):\n        return {\n            \"success\": False,\n            \"message\": \"I'm handling a lot of requests right now. Please try again in a moment.\",\n            \"retry_after\": error.retry_after or 30\n        }\n    \n    if isinstance(error, ToolTimeoutError):\n        return {\n            \"success\": False,\n            \"message\": f\"I couldn't complete that action in time. Would you like me to try again?\",\n            \"action\": \"retry_available\"\n        }\n    \n    if isinstance(error, ValidationError):\n        return {\n            \"success\": False,\n            \"message\": f\"I need more information: {error.user_hint}\",\n            \"action\": \"clarification_needed\"\n        }\n    \n    # Unknown errors - log internally, give generic response\n    logger.error(f\"Unhandled error: {error}\", exc_info=True)\n    return {\n        \"success\": False,\n        \"message\": \"I encountered an unexpected issue. Our team has been notified.\",\n        \"action\": \"escalate\"\n    }\n```\n\n## Timeout Management\n\n```python\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor, TimeoutError\n\nasync def invoke_with_timeout(func, timeout_seconds: float, *args, **kwargs):\n    \"\"\"Run function with timeout, return fallback on timeout.\"\"\"\n    try:\n        return await asyncio.wait_for(\n            asyncio.to_thread(func, *args, **kwargs),\n            timeout=timeout_seconds\n        )\n    except asyncio.TimeoutError:\n        logger.warning(f\"{func.__name__} timed out after {timeout_seconds}s\")\n        raise ToolTimeoutError(f\"{func.__name__} timed out\")\n\n# Tiered timeouts based on operation type\nTIMEOUTS = {\n    \"model_inference\": 30,\n    \"tool_execution\": 10,\n    \"cache_lookup\": 2,\n    \"external_api\": 15\n}\n\nasync def execute_tool(tool_name: str, *args):\n    timeout = TIMEOUTS.get(\"tool_execution\", 10)\n    return await invoke_with_timeout(tools[tool_name], timeout, *args)\n```\n\n## Resilience Checklist\n\n```\n[ ] Retry Logic\n    [ ] Exponential backoff implemented\n    [ ] Jitter added to prevent thundering herd\n    [ ] Retryable vs non-retryable errors distinguished\n    [ ] Max retries capped\n\n[ ] Graceful Degradation\n    [ ] Core vs enhanced features identified\n    [ ] Fallback responses for each failure mode\n    [ ] Degraded mode communicated to users\n\n[ ] Circuit Breakers\n    [ ] Implemented for external dependencies\n    [ ] Thresholds tuned based on traffic\n    [ ] Recovery tested\n\n[ ] Error Handling\n    [ ] User-friendly error messages\n    [ ] Internal errors logged with context\n    [ ] Retry guidance provided to users\n\n[ ] Timeouts\n    [ ] All external calls have timeouts\n    [ ] Timeouts tuned per operation type\n    [ ] Timeout errors handled gracefully\n```\n\n## Summary\n\nResilient agents:\n\n1. **Retry intelligently** - Exponential backoff with jitter for transient failures\n2. **Degrade gracefully** - Continue with reduced functionality when possible\n3. **Fail fast** - Circuit breakers prevent cascade failures\n4. **Communicate clearly** - User-friendly error messages, not stack traces\n5. **Timeout everything** - No unbounded waits for external calls"
}
