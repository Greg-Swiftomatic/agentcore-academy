{
  "title": "Cost Optimization",
  "objectives": [
    "Identify the primary cost drivers in AgentCore deployments",
    "Implement token optimization strategies to reduce LLM costs",
    "Configure appropriate compute and memory settings for cost efficiency"
  ],
  "content": "# Cost Optimization for AgentCore\n\nAI agents can be expensive to operate. Understanding cost drivers and optimization strategies is essential for sustainable production deployments.\n\n## Understanding Cost Drivers\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    AgentCore Cost Breakdown                      │\n│                                                                  │\n│  ┌────────────────────────────────────────────────────────────┐ │\n│  │  LLM Token Costs (typically 60-80% of total)               │ │\n│  │  - Input tokens (prompts, context, tool results)           │ │\n│  │  - Output tokens (responses, reasoning)                    │ │\n│  └────────────────────────────────────────────────────────────┘ │\n│                                                                  │\n│  ┌────────────────────────────────────────────────────────────┐ │\n│  │  Compute Costs (15-25%)                                    │ │\n│  │  - Runtime container execution time                        │ │\n│  │  - Memory allocation                                       │ │\n│  └────────────────────────────────────────────────────────────┘ │\n│                                                                  │\n│  ┌────────────────────────────────────────────────────────────┐ │\n│  │  Storage & Data Transfer (5-15%)                           │ │\n│  │  - Memory service storage                                  │ │\n│  │  - Data transfer between services                          │ │\n│  └────────────────────────────────────────────────────────────┘ │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Token Optimization Strategies\n\nSince LLM tokens are the biggest cost driver, optimizing token usage has the highest impact:\n\n### 1. Efficient System Prompts\n\n```python\n# BAD: Verbose, repetitive system prompt (500+ tokens)\nsystem_prompt_bad = \"\"\"\nYou are a helpful assistant that helps users with their orders.\nYou should always be polite and professional.\nYou have access to tools that can help you look up order information.\nWhen a user asks about an order, you should use the get_order tool.\nYou should always confirm the order number before looking it up.\nMake sure to be thorough in your responses.\nAlways thank the user for their patience.\n...(continues for 500+ tokens)\n\"\"\"\n\n# GOOD: Concise, structured system prompt (100 tokens)\nsystem_prompt_good = \"\"\"\nRole: Order support agent\nTools: get_order(order_id), update_order(order_id, updates)\nBehavior:\n- Confirm order ID before lookup\n- Be concise but complete\n- Escalate to human for refunds > $100\n\"\"\"\n```\n\n### 2. Context Window Management\n\n```python\nfrom agentcore import Memory\n\nmemory = Memory()\n\ndef get_optimized_context(session_id: str, max_tokens: int = 2000):\n    \"\"\"Retrieve only relevant context, not full history.\"\"\"\n    \n    # Get recent messages (sliding window)\n    recent = memory.get_messages(\n        session_id=session_id,\n        limit=10  # Last 10 messages, not all\n    )\n    \n    # Summarize older context instead of including verbatim\n    if memory.get_message_count(session_id) > 10:\n        summary = memory.get_summary(session_id)  # Pre-computed summary\n        context = f\"Previous context: {summary}\\n\\nRecent:\\n\"\n    else:\n        context = \"\"\n    \n    return context + format_messages(recent)\n```\n\n### 3. Tool Result Compression\n\n```python\ndef get_order_status(order_id: str) -> dict:\n    \"\"\"Return only what the LLM needs, not the full API response.\"\"\"\n    \n    # Full API response (500+ tokens)\n    full_response = order_api.get_order(order_id)\n    \n    # Compressed for LLM (50 tokens)\n    return {\n        \"order_id\": full_response[\"id\"],\n        \"status\": full_response[\"status\"],\n        \"eta\": full_response[\"estimated_delivery\"],\n        \"items_count\": len(full_response[\"items\"])\n        # Omit: full item details, internal IDs, audit logs, etc.\n    }\n```\n\n### 4. Model Selection by Task\n\n```python\ndef select_model(task_complexity: str) -> str:\n    \"\"\"Use cheaper models for simple tasks.\"\"\"\n    \n    models = {\n        \"simple\": \"anthropic.claude-3-haiku\",    # $0.25/1M tokens\n        \"medium\": \"anthropic.claude-3-sonnet\",   # $3/1M tokens  \n        \"complex\": \"anthropic.claude-3-opus\"     # $15/1M tokens\n    }\n    \n    return models.get(task_complexity, \"medium\")\n\n# Route simple queries to cheaper models\ndef route_request(prompt: str):\n    if is_simple_lookup(prompt):  # \"What's my order status?\"\n        return invoke_agent(prompt, model=select_model(\"simple\"))\n    elif requires_reasoning(prompt):  # Complex analysis\n        return invoke_agent(prompt, model=select_model(\"complex\"))\n    else:\n        return invoke_agent(prompt, model=select_model(\"medium\"))\n```\n\n## Compute Optimization\n\n### Right-Size Memory Allocation\n\n```python\n# Agent configuration\nconfig = {\n    \"runtime\": {\n        # Start conservative, increase based on metrics\n        \"memory_mb\": 512,  # Not 2048 by default\n        \"timeout_seconds\": 30,  # Not 300 by default\n    }\n}\n\n# Monitor and adjust based on actual usage\n# CloudWatch metrics: MemoryUtilization, Duration\n```\n\n### Batch Processing for Non-Interactive Workloads\n\n```python\nimport asyncio\n\nasync def process_batch(items: list, batch_size: int = 10):\n    \"\"\"Process multiple items efficiently.\"\"\"\n    \n    results = []\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        # Process batch concurrently\n        batch_results = await asyncio.gather(*[\n            process_item(item) for item in batch\n        ])\n        results.extend(batch_results)\n        \n        # Brief pause to avoid throttling\n        await asyncio.sleep(0.1)\n    \n    return results\n```\n\n## Caching Strategies\n\n### Cache Repeated Queries\n\n```python\nimport hashlib\nfrom functools import lru_cache\n\n@lru_cache(maxsize=1000)\ndef get_cached_response(prompt_hash: str):\n    \"\"\"Cache identical prompts.\"\"\"\n    # Lookup in cache\n    cached = cache.get(prompt_hash)\n    if cached:\n        return cached\n    return None\n\ndef invoke_with_cache(prompt: str):\n    prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()\n    \n    cached = get_cached_response(prompt_hash)\n    if cached:\n        return cached  # Free!\n    \n    response = invoke_agent(prompt)\n    cache.set(prompt_hash, response, ttl=3600)  # Cache for 1 hour\n    return response\n```\n\n### Cache Tool Results\n\n```python\ndef get_product_info(product_id: str) -> dict:\n    \"\"\"Cache product info that doesn't change frequently.\"\"\"\n    \n    cache_key = f\"product:{product_id}\"\n    cached = redis.get(cache_key)\n    \n    if cached:\n        return json.loads(cached)\n    \n    product = product_api.get(product_id)\n    redis.setex(cache_key, 3600, json.dumps(product))  # 1 hour TTL\n    return product\n```\n\n## Cost Monitoring\n\n```python\nimport boto3\n\ncloudwatch = boto3.client('cloudwatch')\n\n# Track cost-related metrics\ndef log_invocation_cost(tokens_in: int, tokens_out: int, model: str):\n    # Model pricing (per 1M tokens)\n    pricing = {\n        \"claude-3-haiku\": {\"in\": 0.25, \"out\": 1.25},\n        \"claude-3-sonnet\": {\"in\": 3.0, \"out\": 15.0},\n    }\n    \n    model_price = pricing.get(model, {\"in\": 3.0, \"out\": 15.0})\n    cost = (tokens_in * model_price[\"in\"] + tokens_out * model_price[\"out\"]) / 1_000_000\n    \n    cloudwatch.put_metric_data(\n        Namespace='AgentCore/Costs',\n        MetricData=[{\n            'MetricName': 'InvocationCost',\n            'Value': cost,\n            'Unit': 'None',\n            'Dimensions': [{'Name': 'Model', 'Value': model}]\n        }]\n    )\n```\n\n## Cost Optimization Checklist\n\n```\n[ ] Token Optimization\n    [ ] System prompts under 200 tokens\n    [ ] Context window managed (summarization, sliding window)\n    [ ] Tool results compressed\n    [ ] Appropriate model routing by task complexity\n\n[ ] Compute Optimization\n    [ ] Memory sized based on actual usage\n    [ ] Timeouts set appropriately\n    [ ] Batch processing for offline workloads\n\n[ ] Caching\n    [ ] Response caching for repeated queries\n    [ ] Tool result caching where appropriate\n    [ ] Cache TTLs balanced (freshness vs. cost)\n\n[ ] Monitoring\n    [ ] Per-invocation cost tracking\n    [ ] Budget alerts configured\n    [ ] Regular cost reviews scheduled\n```\n\n## Summary\n\nKey cost optimization strategies:\n\n1. **Optimize tokens** - Biggest cost driver. Compress prompts, manage context, select right models\n2. **Right-size compute** - Don't over-provision memory and timeout\n3. **Cache aggressively** - Repeated queries and stable tool results\n4. **Monitor continuously** - You can't optimize what you don't measure"
}
