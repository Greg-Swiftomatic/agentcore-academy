{
  "title": "Monitoring & Observability",
  "objectives": [
    "Set up CloudWatch dashboards for AgentCore metrics",
    "Implement distributed tracing with OpenTelemetry",
    "Configure alerts for operational and quality metrics"
  ],
  "content": "# Monitoring & Observability\n\nProduction agents require comprehensive monitoring. You need visibility into what your agent is doing, how well it's performing, and when things go wrong.\n\n## The Observability Stack\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    Observability Layers                          │\n│                                                                  │\n│  ┌────────────────────────────────────────────────────────────┐ │\n│  │  METRICS (What's happening?)                               │ │\n│  │  - Invocation counts, latency, error rates                 │ │\n│  │  - Token usage, session counts                             │ │\n│  │  - CloudWatch metrics                                      │ │\n│  └────────────────────────────────────────────────────────────┘ │\n│                                                                  │\n│  ┌────────────────────────────────────────────────────────────┐ │\n│  │  TRACES (How did it happen?)                               │ │\n│  │  - Request flow through agent                              │ │\n│  │  - Tool invocations and timing                             │ │\n│  │  - OpenTelemetry / X-Ray                                   │ │\n│  └────────────────────────────────────────────────────────────┘ │\n│                                                                  │\n│  ┌────────────────────────────────────────────────────────────┐ │\n│  │  LOGS (What exactly happened?)                             │ │\n│  │  - Structured event logs                                   │ │\n│  │  - Debug information                                       │ │\n│  │  - CloudWatch Logs                                         │ │\n│  └────────────────────────────────────────────────────────────┘ │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Key Metrics to Monitor\n\n### Operational Metrics\n\n| Metric | What It Tells You | Alert Threshold |\n|--------|-------------------|----------------|\n| Invocation Count | Traffic volume | Sudden drops/spikes |\n| Latency (p50, p95, p99) | User experience | p95 > 5s |\n| Error Rate | Reliability | > 1% |\n| Token Usage | Cost and complexity | Anomalous spikes |\n| Concurrent Sessions | Capacity utilization | Near limits |\n\n### Quality Metrics\n\n| Metric | What It Tells You | How to Measure |\n|--------|-------------------|----------------|\n| Task Success Rate | Agent effectiveness | User feedback, automated checks |\n| Tool Selection Accuracy | Reasoning quality | Compare expected vs actual tools |\n| Conversation Length | Efficiency | Turns to resolution |\n| Escalation Rate | Agent limitations | Handoffs to humans |\n\n## CloudWatch Integration\n\nAgentCore automatically publishes metrics to CloudWatch:\n\n```python\nimport boto3\nimport json\n\ncloudwatch = boto3.client('cloudwatch')\n\n# Built-in metrics (automatic)\n# Namespace: AWS/BedrockAgentCore\n# - Invocations\n# - Duration\n# - Errors\n# - TokensConsumed\n\n# Custom metrics (you implement)\ndef log_custom_metric(metric_name: str, value: float, dimensions: dict):\n    cloudwatch.put_metric_data(\n        Namespace='AgentCore/Custom',\n        MetricData=[{\n            'MetricName': metric_name,\n            'Value': value,\n            'Unit': 'Count',\n            'Dimensions': [\n                {'Name': k, 'Value': v} for k, v in dimensions.items()\n            ]\n        }]\n    )\n\n# Example: Track tool usage\nlog_custom_metric(\n    'ToolInvocation',\n    value=1,\n    dimensions={'ToolName': 'get_order_status', 'AgentId': 'order-agent'}\n)\n```\n\n### Creating Dashboards\n\n```python\ndashboard_body = {\n    \"widgets\": [\n        {\n            \"type\": \"metric\",\n            \"x\": 0, \"y\": 0, \"width\": 12, \"height\": 6,\n            \"properties\": {\n                \"metrics\": [\n                    [\"AWS/BedrockAgentCore\", \"Invocations\", \"AgentId\", \"my-agent\"]\n                ],\n                \"title\": \"Invocations\",\n                \"period\": 60,\n                \"stat\": \"Sum\"\n            }\n        },\n        {\n            \"type\": \"metric\",\n            \"x\": 12, \"y\": 0, \"width\": 12, \"height\": 6,\n            \"properties\": {\n                \"metrics\": [\n                    [\"AWS/BedrockAgentCore\", \"Duration\", \"AgentId\", \"my-agent\", {\"stat\": \"p50\"}],\n                    [\"...\", {\"stat\": \"p95\"}],\n                    [\"...\", {\"stat\": \"p99\"}]\n                ],\n                \"title\": \"Latency Percentiles\",\n                \"period\": 60\n            }\n        },\n        {\n            \"type\": \"metric\",\n            \"x\": 0, \"y\": 6, \"width\": 12, \"height\": 6,\n            \"properties\": {\n                \"metrics\": [\n                    [{\"expression\": \"errors/invocations*100\", \"label\": \"Error Rate %\"}],\n                    [\"AWS/BedrockAgentCore\", \"Errors\", \"AgentId\", \"my-agent\", {\"id\": \"errors\", \"visible\": false}],\n                    [\".\", \"Invocations\", \".\", \".\", {\"id\": \"invocations\", \"visible\": false}]\n                ],\n                \"title\": \"Error Rate\"\n            }\n        }\n    ]\n}\n\ncloudwatch.put_dashboard(\n    DashboardName='AgentCore-Production',\n    DashboardBody=json.dumps(dashboard_body)\n)\n```\n\n## Distributed Tracing\n\nTracing shows the complete flow of a request through your agent:\n\n```\n[Agent Invoke] 1200ms\n├── [Parse Input] 5ms\n├── [Load Context] 50ms\n│   └── [Memory.get] 45ms\n├── [LLM Inference] 800ms\n├── [Tool: get_order] 200ms\n│   ├── [API Call] 180ms\n│   └── [Parse Response] 20ms\n├── [LLM Inference] 100ms\n└── [Format Response] 45ms\n```\n\n### OpenTelemetry Setup\n\n```python\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n\n# Configure tracer provider\nprovider = TracerProvider()\nprovider.add_span_processor(\n    BatchSpanProcessor(OTLPSpanExporter(\n        endpoint=\"your-collector-endpoint:4317\"\n    ))\n)\ntrace.set_tracer_provider(provider)\n\ntracer = trace.get_tracer(\"agentcore-app\")\n\n# Instrument your agent code\n@app.entrypoint\ndef invoke(payload):\n    with tracer.start_as_current_span(\"agent-invoke\") as span:\n        span.set_attribute(\"user_id\", payload.get(\"user_id\"))\n        span.set_attribute(\"session_id\", payload.get(\"session_id\"))\n        \n        # Each operation creates a child span\n        with tracer.start_as_current_span(\"load-context\"):\n            context = memory.get_context(payload[\"session_id\"])\n        \n        with tracer.start_as_current_span(\"llm-inference\") as llm_span:\n            response = model.invoke(context, payload[\"prompt\"])\n            llm_span.set_attribute(\"tokens_in\", response.input_tokens)\n            llm_span.set_attribute(\"tokens_out\", response.output_tokens)\n        \n        span.set_attribute(\"status\", \"success\")\n        return response\n```\n\n### Tracing Tools\n\n```python\ndef instrumented_tool(func):\n    \"\"\"Decorator to trace tool invocations.\"\"\"\n    def wrapper(*args, **kwargs):\n        with tracer.start_as_current_span(f\"tool:{func.__name__}\") as span:\n            span.set_attribute(\"tool.name\", func.__name__)\n            span.set_attribute(\"tool.args\", str(kwargs))\n            \n            try:\n                result = func(*args, **kwargs)\n                span.set_attribute(\"tool.status\", \"success\")\n                return result\n            except Exception as e:\n                span.set_attribute(\"tool.status\", \"error\")\n                span.set_attribute(\"tool.error\", str(e))\n                raise\n    return wrapper\n\n@instrumented_tool\ndef get_order_status(order_id: str) -> dict:\n    return order_api.get(order_id)\n```\n\n## Alerting\n\n### Configure Critical Alerts\n\n```python\n# High error rate alert\ncloudwatch.put_metric_alarm(\n    AlarmName='AgentCore-HighErrorRate',\n    MetricName='Errors',\n    Namespace='AWS/BedrockAgentCore',\n    Dimensions=[{'Name': 'AgentId', 'Value': 'my-agent'}],\n    Statistic='Sum',\n    Period=300,  # 5 minutes\n    EvaluationPeriods=2,  # 2 consecutive periods\n    Threshold=10,\n    ComparisonOperator='GreaterThanThreshold',\n    AlarmActions=['arn:aws:sns:us-east-1:123456789:oncall-alerts'],\n    TreatMissingData='notBreaching'\n)\n\n# High latency alert\ncloudwatch.put_metric_alarm(\n    AlarmName='AgentCore-HighLatency',\n    MetricName='Duration',\n    Namespace='AWS/BedrockAgentCore',\n    Dimensions=[{'Name': 'AgentId', 'Value': 'my-agent'}],\n    ExtendedStatistic='p95',\n    Period=300,\n    EvaluationPeriods=3,\n    Threshold=5000,  # 5 seconds\n    ComparisonOperator='GreaterThanThreshold',\n    AlarmActions=['arn:aws:sns:us-east-1:123456789:oncall-alerts']\n)\n\n# Anomaly detection for traffic\ncloudwatch.put_anomaly_detector(\n    Namespace='AWS/BedrockAgentCore',\n    MetricName='Invocations',\n    Dimensions=[{'Name': 'AgentId', 'Value': 'my-agent'}],\n    Stat='Sum'\n)\n```\n\n### Alert Best Practices\n\n| Do | Don't |\n|----|-------|\n| Alert on user-impacting issues | Alert on every error |\n| Use anomaly detection for traffic | Set static thresholds for variable metrics |\n| Include runbook links in alerts | Send alerts without context |\n| Page for critical, email for warning | Treat all alerts as critical |\n\n## Structured Logging\n\n```python\nimport logging\nimport json\nfrom datetime import datetime\n\nclass StructuredLogger:\n    def __init__(self, agent_id: str):\n        self.agent_id = agent_id\n        self.logger = logging.getLogger('agentcore')\n        self.logger.setLevel(logging.INFO)\n    \n    def log(self, event: str, **kwargs):\n        record = {\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"agent_id\": self.agent_id,\n            \"event\": event,\n            **kwargs\n        }\n        self.logger.info(json.dumps(record))\n\nlogger = StructuredLogger(\"order-agent\")\n\n# Usage\nlogger.log(\"invocation_start\", \n    user_id=\"user-123\",\n    session_id=\"sess-456\",\n    prompt_length=150\n)\n\nlogger.log(\"tool_called\",\n    tool=\"get_order_status\",\n    duration_ms=180,\n    success=True\n)\n\nlogger.log(\"invocation_complete\",\n    duration_ms=1200,\n    tokens_in=500,\n    tokens_out=150,\n    tools_used=[\"get_order_status\"]\n)\n```\n\n## Observability Checklist\n\n```\n[ ] Metrics\n    [ ] CloudWatch dashboard created\n    [ ] Key metrics visualized (invocations, latency, errors, tokens)\n    [ ] Custom metrics for business KPIs\n\n[ ] Tracing\n    [ ] OpenTelemetry configured\n    [ ] Agent entry point instrumented\n    [ ] All tools traced\n    [ ] Trace sampling configured for high volume\n\n[ ] Logs\n    [ ] Structured logging implemented\n    [ ] Log levels appropriate (not everything is ERROR)\n    [ ] Retention policy set\n\n[ ] Alerts\n    [ ] Error rate alerts\n    [ ] Latency alerts\n    [ ] Anomaly detection for traffic\n    [ ] Runbooks linked to alerts\n```\n\n## Summary\n\nEffective monitoring requires:\n\n1. **Metrics** - Track what's happening (CloudWatch)\n2. **Traces** - Understand how requests flow (OpenTelemetry)\n3. **Logs** - Debug specific issues (structured logging)\n4. **Alerts** - Know when things break (before users tell you)\n\nInvest in observability before you need it - debugging production issues without visibility is painful."
}
