{
  "title": "Runtime Service Deep Dive",
  "objectives": [
    "Understand how the Runtime Service executes agents",
    "Learn about the tool execution pipeline",
    "Master the agent orchestration loop"
  ],
  "content": "# Runtime Service Deep Dive\n\nThe Runtime Service is where the magic happens. It's responsible for executing agents, coordinating tool calls, and managing the interaction between your application and the LLM.\n\n## Architecture\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                     Runtime Service                          │\n│                                                              │\n│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐       │\n│  │   Request    │  │    Agent     │  │   Response   │       │\n│  │   Handler    │──│  Orchestrator│──│   Streamer   │       │\n│  └──────────────┘  └──────┬───────┘  └──────────────┘       │\n│                           │                                  │\n│            ┌──────────────┼──────────────┐                  │\n│            ▼              ▼              ▼                  │\n│     ┌───────────┐  ┌───────────┐  ┌───────────┐            │\n│     │   Tool    │  │    LLM    │  │  Context  │            │\n│     │  Executor │  │  Client   │  │  Builder  │            │\n│     └───────────┘  └───────────┘  └───────────┘            │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## The Orchestration Loop\n\nThe core of the Runtime is the orchestration loop:\n\n```python\nasync def run_agent(session_id: str, user_message: str):\n    # 1. Load context\n    context = await memory.get_context(session_id)\n    \n    # 2. Build messages for LLM\n    messages = build_messages(context, user_message)\n    \n    # 3. Enter the agent loop\n    while True:\n        # Call the LLM\n        response = await llm.invoke(messages)\n        \n        # Check if tool use is requested\n        if response.has_tool_calls:\n            # Execute tools\n            for tool_call in response.tool_calls:\n                result = await execute_tool(tool_call)\n                messages.append(tool_result(result))\n            # Continue loop with tool results\n            continue\n        \n        # No more tools - we have the final response\n        break\n    \n    # 4. Save updated context\n    await memory.save_context(session_id, messages)\n    \n    return response.content\n```\n\n## Tool Execution\n\n### Tool Definition\n\nTools are defined with a schema that the LLM understands:\n\n```python\n{\n    \"name\": \"search_database\",\n    \"description\": \"Search the product database\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\n                \"type\": \"string\",\n                \"description\": \"Search query\"\n            },\n            \"limit\": {\n                \"type\": \"integer\",\n                \"description\": \"Max results\",\n                \"default\": 10\n            }\n        },\n        \"required\": [\"query\"]\n    }\n}\n```\n\n### Execution Pipeline\n\n```\nTool Call from LLM\n       │\n       ▼\n┌──────────────┐\n│  Validate    │──► Check parameters match schema\n│  Parameters  │\n└──────┬───────┘\n       │\n       ▼\n┌──────────────┐\n│   Check      │──► Verify agent has permission\n│  Permissions │\n└──────┬───────┘\n       │\n       ▼\n┌──────────────┐\n│   Execute    │──► Run in isolated environment\n│   Handler    │\n└──────┬───────┘\n       │\n       ▼\n┌──────────────┐\n│   Format     │──► Convert result for LLM\n│   Result     │\n└──────────────┘\n```\n\n### Tool Isolation\n\nTools run in isolated environments for security:\n\n```python\n# Tools execute in a sandboxed context\nasync def execute_tool(tool_call: ToolCall) -> ToolResult:\n    async with ToolSandbox() as sandbox:\n        # Resource limits\n        sandbox.set_timeout(30)  # seconds\n        sandbox.set_memory_limit(256)  # MB\n        \n        # Network restrictions\n        sandbox.allow_hosts([\"api.internal.com\"])\n        \n        # Execute\n        result = await sandbox.run(tool_call)\n        \n        return result\n```\n\n## Streaming Responses\n\nThe Runtime supports streaming for responsive UIs:\n\n```python\nasync def stream_agent(session_id: str, message: str):\n    context = await memory.get_context(session_id)\n    messages = build_messages(context, message)\n    \n    async for event in llm.stream(messages):\n        if event.type == \"text_delta\":\n            yield StreamEvent(\"text\", event.text)\n        elif event.type == \"tool_use\":\n            yield StreamEvent(\"tool_start\", event.tool_name)\n            result = await execute_tool(event)\n            yield StreamEvent(\"tool_result\", result)\n            # Continue streaming with result\n            messages.append(tool_result(result))\n        elif event.type == \"end\":\n            yield StreamEvent(\"done\", None)\n```\n\n## Error Handling\n\nRobust error handling is critical:\n\n```python\nclass RuntimeErrorHandler:\n    async def handle_llm_error(self, error):\n        if is_rate_limit(error):\n            await backoff_and_retry()\n        elif is_context_too_long(error):\n            await summarize_and_retry()\n        else:\n            raise AgentError(\"LLM call failed\", error)\n    \n    async def handle_tool_error(self, error, tool_call):\n        # Return error as tool result - let agent decide\n        return ToolResult(\n            tool_call_id=tool_call.id,\n            output=f\"Error: {error.message}\",\n            is_error=True\n        )\n```\n\n## Performance Optimization\n\n### Request Coalescing\n\nMultiple tool calls can execute in parallel:\n\n```python\nasync def execute_tools_parallel(tool_calls: list):\n    # Group independent tools\n    tasks = [execute_tool(tc) for tc in tool_calls]\n    results = await asyncio.gather(*tasks)\n    return results\n```\n\n### Context Caching\n\nReduce redundant LLM calls:\n\n```python\n# Cache partial context for multi-turn conversations\ncache_key = hash(system_prompt + history[-5:])\nif cache_key in context_cache:\n    # Reuse cached context embedding\n    context = context_cache[cache_key]\n```\n\n## Monitoring & Observability\n\nThe Runtime emits detailed telemetry:\n\n```python\n# Key metrics\nmetrics = {\n    \"agent_latency_ms\": Histogram,\n    \"tool_execution_time_ms\": Histogram,\n    \"llm_tokens_used\": Counter,\n    \"tool_calls_total\": Counter,\n    \"errors_total\": Counter,\n}\n\n# Trace spans\nwith tracer.span(\"agent_execution\"):\n    with tracer.span(\"llm_call\"):\n        response = await llm.invoke(...)\n    with tracer.span(\"tool_execution\"):\n        result = await execute_tool(...)\n```\n\n## Next Steps\n\nYou now understand how the Runtime executes agents. Next, we'll explore the Memory Service and how agents maintain context across conversations."
}
