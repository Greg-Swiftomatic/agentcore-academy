{
  "title": "Testing and Iteration",
  "objectives": [
    "Create comprehensive test suites for your agent",
    "Evaluate agent performance with metrics and user testing",
    "Iterate on improvements based on real feedback"
  ],
  "content": "# Testing and Iteration\n\nYour agent works - but does it work *well*? Before deployment, you need to rigorously test and improve it.\n\nThis lesson covers testing strategies, evaluation metrics, and the iteration loop that turns a working prototype into a production-ready agent.\n\n## The Testing Pyramid for Agents\n\n```\n                    ┌───────────────┐\n                    │   End-to-End  │   Few but critical\n                    │     Tests     │   (real conversations)\n                   ┌┴───────────────┴┐\n                   │   Integration   │   More tests\n                   │      Tests      │   (tools + agent)\n                  ┌┴─────────────────┴┐\n                  │     Unit Tests    │   Many tests\n                  │   (tools, logic)  │   (fast, isolated)\n                  └───────────────────┘\n```\n\n## Level 1: Unit Tests\n\nTest each tool in isolation:\n\n```python\n# tests/test_tools.py\nimport pytest\nfrom src.tools.my_tools import search_faq, get_order_status\n\nclass TestSearchFaq:\n    def test_finds_matching_entry(self):\n        results = search_faq(\"return policy\")\n        assert len(results) > 0\n        assert \"return\" in results[0][\"answer\"].lower()\n    \n    def test_respects_category_filter(self):\n        results = search_faq(\"shipping\", category=\"shipping\")\n        # All results should be from shipping category\n        for result in results:\n            assert \"ship\" in result[\"answer\"].lower()\n    \n    def test_returns_empty_for_no_match(self):\n        results = search_faq(\"xyzzy nonsense query\")\n        assert results == []\n    \n    def test_respects_max_results(self):\n        results = search_faq(\"product\", max_results=1)\n        assert len(results) <= 1\n\nclass TestGetOrderStatus:\n    def test_valid_order_format(self):\n        result = get_order_status(\"ORD-123456\")\n        assert \"error\" not in result or \"not found\" in result.get(\"error\", \"\")\n    \n    def test_invalid_order_format(self):\n        result = get_order_status(\"invalid\")\n        assert \"error\" in result\n        assert \"format\" in result[\"error\"].lower()\n    \n    def test_handles_timeout(self, mocker):\n        mocker.patch('requests.get', side_effect=requests.Timeout)\n        result = get_order_status(\"ORD-123456\")\n        assert \"timeout\" in result[\"error\"].lower()\n```\n\n## Level 2: Integration Tests\n\nTest that the agent uses tools correctly:\n\n```python\n# tests/test_integration.py\nimport pytest\nfrom src.agent import MyCapstoneAgent\n\nclass TestAgentIntegration:\n    @pytest.fixture\n    def agent(self):\n        return MyCapstoneAgent()\n    \n    def test_uses_faq_for_policy_question(self, agent):\n        response = agent.chat(\"What is your return policy?\")\n        # Should have used search_faq and found relevant info\n        assert any(word in response.lower() for word in [\"return\", \"30 days\", \"refund\"])\n    \n    def test_uses_order_tool_for_status(self, agent):\n        response = agent.chat(\"What's the status of order ORD-123456?\")\n        # Should attempt to look up the order\n        assert any(word in response.lower() for word in [\"order\", \"status\", \"looking\"])\n    \n    def test_maintains_context(self, agent):\n        session_id = \"test-session\"\n        agent.chat(\"My order number is ORD-123456\", session_id)\n        response = agent.chat(\"What's the status?\", session_id)\n        # Should remember the order number\n        assert \"ORD-123456\" in response or \"order\" in response.lower()\n    \n    def test_handles_unknown_question(self, agent):\n        response = agent.chat(\"What's the meaning of life?\")\n        # Should gracefully decline or redirect\n        assert any(phrase in response.lower() for phrase in [\n            \"don't have information\",\n            \"can't help with that\",\n            \"outside my scope\",\n            \"focus on\"\n        ])\n```\n\n## Level 3: End-to-End Tests\n\nTest realistic conversation flows:\n\n```python\n# tests/test_e2e.py\nimport pytest\nfrom src.agent import MyCapstoneAgent\n\nclass TestConversationFlows:\n    def test_complete_support_flow(self):\n        \"\"\"Test a realistic multi-turn support conversation.\"\"\"\n        agent = MyCapstoneAgent()\n        session = \"e2e-test\"\n        \n        # Turn 1: Greeting\n        r1 = agent.chat(\"Hi, I need help with a return\", session)\n        assert r1  # Got a response\n        \n        # Turn 2: Provide order number\n        r2 = agent.chat(\"My order is ORD-789012\", session)\n        assert \"order\" in r2.lower()  # Acknowledged the order\n        \n        # Turn 3: Ask about return process\n        r3 = agent.chat(\"How do I return this item?\", session)\n        assert any(word in r3.lower() for word in [\"return\", \"ship\", \"refund\"])\n        \n        # Turn 4: Ask follow-up\n        r4 = agent.chat(\"Will I get a full refund?\", session)\n        assert \"refund\" in r4.lower()\n\n    def test_escalation_flow(self):\n        \"\"\"Test that complex issues get escalated.\"\"\"\n        agent = MyCapstoneAgent()\n        session = \"escalation-test\"\n        \n        # Simulate a complex issue\n        r1 = agent.chat(\"I'm extremely frustrated! I've been waiting 3 weeks!\", session)\n        r2 = agent.chat(\"This is unacceptable, I want to speak to someone NOW\", session)\n        \n        # Should offer escalation\n        assert any(phrase in r2.lower() for phrase in [\n            \"human\", \"agent\", \"escalat\", \"ticket\", \"someone will\"\n        ])\n```\n\n## Evaluation Metrics\n\nMeasure your agent's performance:\n\n### Metric 1: Task Completion Rate\n\n```python\ndef evaluate_task_completion(agent, test_cases):\n    \"\"\"\n    Test if agent completes tasks successfully.\n    \n    test_cases: list of {\"input\": str, \"expected_outcome\": str}\n    \"\"\"\n    results = []\n    for case in test_cases:\n        response = agent.chat(case[\"input\"])\n        success = case[\"expected_outcome\"].lower() in response.lower()\n        results.append({\n            \"input\": case[\"input\"],\n            \"response\": response,\n            \"success\": success\n        })\n    \n    success_rate = sum(1 for r in results if r[\"success\"]) / len(results)\n    return success_rate, results\n\n# Example test cases\ntest_cases = [\n    {\"input\": \"What's your return policy?\", \"expected_outcome\": \"30 days\"},\n    {\"input\": \"How long does shipping take?\", \"expected_outcome\": \"5-7\"},\n    {\"input\": \"Can I cancel my order?\", \"expected_outcome\": \"cancel\"},\n]\n\nrate, details = evaluate_task_completion(agent, test_cases)\nprint(f\"Task completion rate: {rate:.1%}\")\n```\n\n### Metric 2: Tool Usage Accuracy\n\n```python\ndef evaluate_tool_usage(agent, tool_test_cases):\n    \"\"\"\n    Test if agent uses the right tools for the right queries.\n    \n    tool_test_cases: list of {\"input\": str, \"expected_tool\": str}\n    \"\"\"\n    # This requires instrumenting your agent to log tool calls\n    results = []\n    for case in tool_test_cases:\n        response, tools_used = agent.chat_with_trace(case[\"input\"])\n        correct = case[\"expected_tool\"] in tools_used\n        results.append({\n            \"input\": case[\"input\"],\n            \"expected\": case[\"expected_tool\"],\n            \"actual\": tools_used,\n            \"correct\": correct\n        })\n    \n    accuracy = sum(1 for r in results if r[\"correct\"]) / len(results)\n    return accuracy, results\n```\n\n### Metric 3: Response Quality (Human Eval)\n\nCreate a simple rubric:\n\n| Dimension | 1 (Poor) | 3 (Acceptable) | 5 (Excellent) |\n|-----------|----------|----------------|---------------|\n| **Accuracy** | Wrong information | Mostly correct | Fully accurate |\n| **Helpfulness** | Doesn't address need | Partially helpful | Solves the problem |\n| **Clarity** | Confusing | Understandable | Clear and well-structured |\n| **Tone** | Robotic/rude | Neutral | Friendly and professional |\n\n```python\ndef human_eval_prompt(conversation):\n    \"\"\"Generate a prompt for human evaluation.\"\"\"\n    return f\"\"\"\n    Rate this agent response on a scale of 1-5:\n    \n    USER: {conversation['user_input']}\n    AGENT: {conversation['agent_response']}\n    \n    Accuracy (1-5): ___\n    Helpfulness (1-5): ___\n    Clarity (1-5): ___\n    Tone (1-5): ___\n    \n    Comments:\n    \"\"\"\n```\n\n## The Iteration Loop\n\nImprovement is iterative. Follow this cycle:\n\n```\n┌─────────────────────────────────────────────────────┐\n│                                                     │\n│    ┌─────────┐    ┌─────────┐    ┌─────────┐      │\n│    │  TEST   │───▶│ ANALYZE │───▶│  FIX    │      │\n│    └────▲────┘    └─────────┘    └────┬────┘      │\n│         │                              │           │\n│         └──────────────────────────────┘           │\n│                                                     │\n└─────────────────────────────────────────────────────┘\n```\n\n### Common Issues and Fixes\n\n| Symptom | Root Cause | Fix |\n|---------|-----------|-----|\n| Agent doesn't use tools | Tool descriptions unclear | Improve descriptions, add examples |\n| Uses wrong tool | Similar tool descriptions | Differentiate purposes clearly |\n| Hallucinations | No grounding | Add retrieval, constrain to known info |\n| Generic responses | Over-reliance on base model | Add specific examples to prompt |\n| Loses context | Memory not working | Check session handling, increase window |\n| Too verbose | No length guidance | Add brevity instructions to prompt |\n| Doesn't escalate | No escalation logic | Add explicit escalation triggers |\n\n### Improving Tool Descriptions\n\n**Before (vague):**\n```\n\"Searches for information\"\n```\n\n**After (specific):**\n```\n\"Searches the FAQ knowledge base for answers to customer questions about \nreturn policies, shipping times, and product information. Use this when \nthe customer asks about company policies or procedures. Returns matching \nFAQ entries - if empty, the question may need human support.\"\n```\n\n### Improving System Prompt\n\nAdd examples of good responses:\n\n```python\nsystem_prompt = \"\"\"\nYou are a customer support agent for Acme Corp.\n\n## Your Role\n- Answer questions about products, shipping, and policies\n- Help customers with order status and returns\n- Escalate complex issues to human support\n\n## Response Style\n- Be friendly and professional\n- Keep responses concise (2-3 sentences for simple questions)\n- Use bullet points for multi-step instructions\n\n## Examples\n\nUser: \"How long does shipping take?\"\nGood response: \"Standard shipping takes 5-7 business days. Express shipping \n(+$10) delivers in 2-3 business days. Would you like tracking for an existing order?\"\n\nUser: \"I'm so frustrated, my order is late!\"\nGood response: \"I'm sorry to hear about the delay - that's frustrating. Let me \nlook into your order right away. Could you share your order number (format: ORD-XXXXXX)?\"\n\"\"\"\n```\n\n## Your Testing Checklist\n\nBefore moving to deployment:\n\n```\n☐ Unit tests pass for all tools\n☐ Integration tests verify tool usage\n☐ At least 3 end-to-end conversation tests\n☐ Task completion rate > 80%\n☐ Tool usage accuracy > 90%\n☐ Tested 10+ edge cases manually\n☐ Had 2+ people try the agent and give feedback\n☐ Incorporated feedback into improvements\n☐ Re-tested after improvements\n```\n\n## Your Assignment\n\n1. **Write unit tests** for each of your tools\n2. **Create 5 end-to-end test cases** representing realistic conversations\n3. **Measure your task completion rate** - aim for >80%\n4. **Have someone else try your agent** and collect feedback\n5. **Make at least 3 improvements** based on testing\n6. **Document what you learned** - what worked, what didn't\n\n## What's Next\n\nYour agent is tested and refined. In the final lesson, you'll deploy it to AgentCore and make it available to real users."
}
